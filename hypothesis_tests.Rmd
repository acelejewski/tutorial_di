---
title: "Hypothesis Testing"
author: "Adam"
date: "May 27, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gridExtra)
```

# Hypothsis testing

In empirical research, we test theories. A good theory, is one that generates testable predictions. To test such predictions, the Null Hypothesis Significance Test (**NHST**) is the most prevalent statistical tool.


###Setting up a statistical hypothesis.

e.g. evaluate drug for obesity

With  NHSTs, we calculate the probability of an effect of a certain size, or larger being observed if there was no true effect. This is done by testing a null hypothesis ($H_0$) rather than the hypothesis of interest, ($H_1$). The null hypothesis, is the hypothesis that there is no effect.

e.g. if we want see that a drug reduces eating, the $H_0$ is that drug and no-drug groups spend the same amount of time lever pressing for food.

A NHST in our example, determines the probability of obtaining a results if our true population difference difference was 0. This probability is the significance probability, or $p$

If $p$, is small enough, than we can deem that it is reasonable to reject $H_0$

Therefore, there are 2 potential outcomes of an NHST, rejecting, or failing to reject the null hypothesis.

There are some subtle distinctions here, lets review the main points

1. Set up an $H_0$
2. Calculate the probability, ($p$) of obtaining the observed statistic if $H_0$ is true
3. If $p$ is small, we reject the $H_0$. If p is large we fail to reject the $H_0$


**A $p$ value is a conditional probability, specifically it is the probability of a result as extreme or more extreme than the one observed when $H_0$ is true.  It is *not* the the probability of $H_0$ being true (an unconditional probability).**

OK, so we reject $H_0$ when $p$ is small, but what is a small p?

###Deciding on a statistical signifance level
NHSTs work only if we set a significance criterion, denoted by $\alpha$. $\alpha$ is set prior to the test. Otherwise we can always say, well, that was close enough.


What $\alpha$ is set to, is specific to various fields. 

For many disciplines a common $\alpha$ level is set at  $\alpha = 0.05$. When $p<0.05$ we say that statistical significance is declared. i.e. our drug works. 


## Calculating  p
In order to a construct significance test, at least 3  components are required.

1. A null hypothesis
2. An $\alpha$ level 
3. A way of calculating $p$


If the data is sampled from a known probability distribution, the $p$ can be calculated exactly. Otherwise, we estimate $p$.

##A one sample z-test
*A basic form of NHST when varaince of distribution is known.* \
In theory, we cannot know the population parameters unless we recorded them from each member of the population.
In practice we can come close enough in  cases where we have a lot of data, e.g. BMI, or IQ. So in practice this can be true.

Say that it is well know rats will spend on an average of 12.5 min out of 30 min session, lever pressing for given concentration of sugar water over a fixed duration of time. \


If we are testing our appetite suppressing drug, our $H_0$ that the drug has no effect:
$$H_0: \mu = 12.5\\ H_1: \mu \neq 12.5\\$$
 

Lets say that we evaluated a sample of $n=16$ rats that were given the  appetite suppressing drug. We found that these rats spend 9.5 min lever pressing for the sugar water  over fixed session duration.  

Lets also say that  the variance for time spent lever pressing for these rats is 2 min. If the distribution of the time spent lever pressing is *normally distributed*, we can obtain obtain a test z-statistic, or z-score. 


$$\frac{(\hat{\mu}-\mu_0)}{\sigma_{\hat{\mu}}}  \;\;\   z \sim N(0,1)$$
Where $\sigma_{\hat{\mu}}$ is the standard error of the mean (SE) and is equivalent to $\sigma/\sqrt{n} = 2/\sqrt{16} = 2$

So for this example:
$$\frac{(\hat{\mu}-\mu_0)}{\sigma_{\hat{\mu}}} = \frac{(9.5-12.5)}{2} = -1.5$$

For a one sided test,  the p value is the probability that we of get a z-statistic more extreme than -1.5 (more negative). For a two sided test, the p value  is the probability that z is  $\leq -1.5$ and $\geq 1.5$. Graphically we can represent the probability of the socre. (within the blue shaded region)

```{r fig.height=3, fig.width=8}

grid.arrange(
ggplot(NULL, aes(c(-5,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "lightblue", xlim = c(-4, -1.5)) +
  geom_area(stat = "function", fun = dnorm, fill = "grey70", xlim = c(-1.5, 4)) +
  geom_area(stat = "function", fun = dnorm, fill = "lightblue", xlim = c(1.5, 4)) +
  ggtitle("a two sided test") +
  xlab("z") +
  ylab("probability density") +
  theme_bw(),

ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "lightblue", xlim = c(-4, 4)) +
  geom_area(stat = "function", fun = dnorm, fill = "gray70", xlim = c(-1.5, 4)) +
  ggtitle("a one sided test") +
  xlab("z") +
  ylab("probability density") +
  theme_bw(),
ncol=2)

```


we can get the probability of $z = -1.5$ in R using the `pnorm` function.


```{r}
pnorm(-1.5)
```
For a one sided test. For a 2 sided test we multiply p by 2. Depending on our chosen $\alpha$ we can reject or fail to reject the null. This decision is influenced by whether the test is one or two-sided. 

Although directional tests produces smaller p-values than non-directional tests, bi-directional tests are recommended.
For tests statistics with symmetrical distribution the observed p value for a one sided directional equivalent will be half of the of the two-sided equivalent. However:

1. What if there is an effect in opposite to our prediction?
2. We need to choose our $\alpha$ in advance



##The t-test
Since we don't usually know the variance of the distribution from which we are drawing our sample we use the t-distribution for our significance test. The t-distribution is better for small *n*'s, and begins to look like the z-distribution as the $n$ increases.

####One sample t-test
An NHST for a single sample is constructed using samplle estimate of $\sigma$. If $H_0$ is true the statistic is:

$$t = \frac{\hat{\mu} - \mu_0}{\hat{\sigma_{\hat{\mu}}}} = \frac{\hat{\mu} - \mu_0}{\hat{\sigma}/\sqrt{n}} = \sim t(v) $$
- where $\mu_0$ is the population mean as specified under $H_0$, and $\hat{\sigma_{\hat{\mu}}}$ is the standard error of the mean.\
- the null hypothesis is $H_0: \mu = \mu_0$  and $H_1: \mu \neq \mu_0$\
- the t statistic is evaluated against the t-distribution with $v = n-1$ degrees of freedom $(df)$



#### Independent t-test
The independent t-test is the NHST of difference between the means, of two *independent normal samples*, with unknown but *equal $\sigma$*

If the null hypothesis is true (no difference between the means) $H_0: \mu_1 - \mu_2$ = 0, where the alternate hypothesis is  $H_A: \mu_1 \neq  \mu_2$

For an independent t test the t-statistic takes the form of:


$$t = \frac{\hat{\mu_1} - \hat{\mu_2} }{\hat{\sigma_{\hat{\mu_1}-\hat{\mu_2}}}} =  \frac{\hat{\mu_1} - \hat{\mu_2} }{\hat{\sigma}_{pooled} \sqrt{1/n_1 + 1/n_2}}$$

This distribution has a df of N-2 where N is the total sample size. 


We generate fake data from our groups by sampling from the normal distribution to conform with our drug example.
```{r}
set.seed(232)
rats_treat <- rnorm(n  = 16, mean = 9.5, sd = 2 )
rats_control <- rnorm(n  = 16, mean = 12.5, sd = 2 ) 
```


We can use the formulas as we did with the z test
```{r}
t.test(rats_treat, rats_control, var.equal = TRUE)
```


