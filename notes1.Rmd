---
title: "bayesnotesI"
author: "Adam"
date: "May 27, 2018"
output:  
      html_document:  
        keep_md: true  
        
        
        
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The null hypothises signifcance testing controversy
NHST hypotheses testing is common to many disciplines, and has faced criticisms.Some have called for classical frequent inference to be abandoned (including NHST and CI in favor of non frequentist alternatives such as likelihood or Bayesian inference=)

The criticism of hypothesis testing can fall into 3 categories

1.  *Signficance testing is widly misunderstood*
2.  *NHST leads to serious prolbems when applied*
3. *NHSTs are the worng tool for the job*: NHST make probability calculations that are conditional on the null hypothesis $H_0$. It is a matter of philosophical debate as to whether $H_0$ can ever be true. 


Most of the misconceptions, involve diffuculites understanding or reasoning with conditional probabilities. 

**A  *p* value is the probability of a result as extreme or more extreme than the one observed when $H_0$ is true. ** It is *not* the the probability of $H_0$ being true (an unconditional probability)


**Lindley's paradox:**  No matter how small  the value $H_0$ can be be highly probable. Scenario: A mother's two children die months apart form each other. She is accused of murder, but the differnece is SIDS.  SIDS are 1/8543, so then the probability of two subsequent deaths is 1 in 73 million - This makes the assumption that the two results are independent. This is a conditional probability. It is the pros ability of the observed data on the assumption that both deaths are due to SIDS i.e.e ($H_0$) is true. The actual probability that both deaths were due to SIDS is larger that one in 73 million. The probability of either cause of death can be evaluated without considering the other. As there are two mutually exclusive outcomes, the probability of either hypothesis depends on the relative rarity of SIDS and murder. If they are equally rare, the ratio of the conditional probabilities is one, and the probability of either being correct is .5. Therefore the probability of either hypothesis cannot be evaluated by looking at the p value alone. 

###Take-home
**Statistical significance, or the p-value,  is a tool for detecting an effect (detecting the signal against the stastical noise). It is *not* a tool for assessing evidence or determingin the importance. **


##Some practical problems with signifcance tests
###NHSTs dont't fucntion properly when statsitcal power is set to low or too hight. 

If a statistical power is set to low or to high, NHSTs don't function properly. A CI is superior to statistical power alone, because the CI  width can provide and indication of precision whit which an effect has been measured.

###Multiple Testing
Type I error, is the probability of rejecting $H_0$ when it is true. The probability of Type I error is equivalent to the significance criterion. As each test has its own Type I error, the error rate rises rapidly with the number of uncorrected tests. Perhaps, a more vexing problem that emerges in research is that as data about a number of variables is collected, even when the original hypothesis is not supported by the data. 

I suspect that many low to mid level scientists survive though successful storytelling. Weaving stories that are interesting enough to warrent publication around their data. A clever explanation rooted in  for an unexpected finding is a key element to good story telling. This in itself is innocuous enough, but is compounded by the lack of replications. 

One way around the issue is by declaring the both hypothesis and the primary outcome measure in advance. Ignoring new interesting trends in data, simply due their unexpected nature  however, may be wasteful


###Stoping Rule
A *p*-value  calculated for a significance test is only accurate if an appropriate stopping rule is in place. 

Sometimes data is collected until $p < \alpha$, or optional stopping. With sufficient resources, this strategy guarantees $p < 0$ even if $H_0$ is true. Sometimes this issue creeps subtle ways e.g. a reviewer asks to collect more data. This is arguable more problematic than multiple testing, in part due to the fact that this is a less apperant problem. 

###Take home
In summary some issues with NHSTs. 

1. neglect of statistical power
2. multiple testing
3. optional stopping

###Simple stopping rules for NHSTs and CIs
This is especially important in clinical trials where it is important to stop early if needed. e.g. Treatment effective, treatment not effective, treatment harmful.

**COAST (composite opent adaptive sequencial test)** Frick proposed fixing an arbitrary min *N* ($N_1$) and a significance \alpha range at which to continue, collecting,  stopping when  $p < \alpha_{lower}$ or $p > \alpha_{higher}$. $N_1$ is incremented in small batches. In simulations it was shown, that a for two  sided *t* test,  a true $\alpha$ close to .05  was maintained when $p < \alpha_{lower} = .01$ and $p < \alpha_{lower} = 0.36$. COAST  is fairly robust to changes in $N_1$, sample size, and incremental increase in $N$. COAST tends to have greater statistical power than fixed-sample rule
**CLAST compostie limted adaptive sequential test** A more efficient version of COAST implementing a stop when $N = N_{max}$ rule. $N_{max}$ is determined according to the fixed sample rule ($N_{FSR}$) with $N_{min}$ set as  ($\frac{1}{2} N_{FSR}$) and $N{max}$ set as  $\frac{3}{2}N_{FSR}$. Data are added in increments of size $N_2$
**CI stopping rule** Keep collecting data until observed width is narrower than the desired width ($W$). Using this approach it important to select a value of W that will always exclude either ES expected under H0 or   effect size you wish to detect

##Can the null hypthesis ever be true?
* Type I errors probably cannot occur for most studies 
* nil hypothesis is always false (Cohen) given a large enough sample size.

The $H_0$ is a single point on a continuum, and therefore  and infinitesimally small point on a line. Therefore the probability of it being true is 0. However, much research serves to identify invariances, e.g. speed of light. For such invariances the $H_0$ might serve as a useful proxy. Therefore a point null hypothesis is plausible. 

There are two arguments as to why setting up a point null hypothesis might be reasonable.

1. While some non-zero effect is almost certainly true, you might not know the direction of the effect. (we can reject the $H_0$ with a 2-sided test)
2. $H_0$ is a reasonable proxy for a negligible effect

##Frequentist responses to the NHST controversy

## Strict Neyman-Pearons inference.
Many problems with application of the NHST arise from the confusion of Fisherian with Neyman-Pearson inference. Neyman and Pearson aimed to develop a description procedure controlling for the lsdong-run Type-1 and Type-II error rates for sets of tests.


1. Desired levels of $\alpha$ and $\beta$ should be determined in relation to the cost of Type-I and Type-II errors. Much research sets an  $\alpha = .05$ for statistical power and arbitrary $\beta = 0.20$ is common (power $1-\beta=.80$)
2. Practices that distort calculated p values addressed: by by adjusting $\alpha$ or adopting a stopping rule)
3. The interpretation of the test: The outcome outcome as in accepting  or reject $H_0$ 



### Suplementing frequentist inference
There are potential fixes to classical frequentist inference.
1. Increased use of CIS as an informal method provides a way to augment or replace NHSTs and p values. This fits well with the philosophy in which formal inference i to understanding of  the data
2. Increased emphasis on effect size as a way to supplement or correct the difference in significance tests. However, this may also be misleading, as under powered studies may produces large effect sizes despite being non-significant. 
3. Meta-analysis and replications

###The statsitcs of replicaiton




